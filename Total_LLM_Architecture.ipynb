{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "w_query=torch.nn.Parameter(torch.randn(d_in,d_out),requires_grad=False)\n",
    "w_key=torch.nn.Parameter(torch.randn(d_in,d_out),requires_grad=False)\n",
    "w_value=torch.nn.Parameter(torch.randn(d_in,d_out),requires_grad=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Parameter containing:\n",
      "tensor([[-0.1115,  0.1204],\n",
      "        [-0.3696, -0.2404],\n",
      "        [-1.1969,  0.2093]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\",w_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with the indivisual word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1729, -0.0048])\n",
      "tensor([-0.1142, -0.7676])\n",
      "tensor([0.4107, 0.6274])\n"
     ]
    }
   ],
   "source": [
    "query_2=x_2 @ w_query\n",
    "key_2=x_2 @ w_key\n",
    "value_2=x_2 @ w_value\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this with the Whole Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1686,  0.2019],\n",
      "        [-1.1729, -0.0048],\n",
      "        [-1.1438, -0.0018],\n",
      "        [-0.6339, -0.0439],\n",
      "        [-0.2979,  0.0535],\n",
      "        [-0.9596, -0.0712]])\n"
     ]
    }
   ],
   "source": [
    "query=inputs @ w_query\n",
    "keys=inputs @ w_key\n",
    "values= inputs @ w_value\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1376)\n"
     ]
    }
   ],
   "source": [
    "keys_2=keys[1]\n",
    "attn_Score=query_2 @ keys_2\n",
    "print(attn_Score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0740, -0.0216,  0.0126, -0.1230,  0.6250, -0.4498],\n",
      "        [ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809],\n",
      "        [ 0.2098,  0.1320,  0.1665, -0.0489,  0.7408, -0.3725],\n",
      "        [ 0.1458,  0.1061,  0.1254, -0.0118,  0.4384, -0.1919],\n",
      "        [ 0.0175, -0.0071,  0.0017, -0.0321,  0.1580, -0.1153],\n",
      "        [ 0.2240,  0.1642,  0.1935, -0.0161,  0.6667, -0.2888]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attn_Score=query @ keys.T\n",
    "print(attn_Score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.1515, 0.1568, 0.1369, 0.2893, 0.0988],\n",
      "        [0.1689, 0.1560, 0.1616, 0.1294, 0.2912, 0.0929],\n",
      "        [0.1691, 0.1564, 0.1619, 0.1305, 0.2876, 0.0945],\n",
      "        [0.1710, 0.1643, 0.1675, 0.1461, 0.2291, 0.1220],\n",
      "        [0.1684, 0.1643, 0.1658, 0.1603, 0.1938, 0.1475],\n",
      "        [0.1708, 0.1609, 0.1657, 0.1344, 0.2659, 0.1023]])\n"
     ]
    }
   ],
   "source": [
    "normalized_attn_Score=torch.nn.functional.softmax(attn_Score,dim=1)\n",
    "print(normalized_attn_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1684, 0.1644, 0.1658, 0.1603, 0.1933, 0.1477],\n",
      "        [0.1692, 0.1658, 0.1673, 0.1583, 0.1938, 0.1457],\n",
      "        [0.1691, 0.1658, 0.1673, 0.1585, 0.1931, 0.1462],\n",
      "        [0.1683, 0.1666, 0.1675, 0.1618, 0.1811, 0.1547],\n",
      "        [0.1672, 0.1662, 0.1665, 0.1651, 0.1732, 0.1617],\n",
      "        [0.1690, 0.1665, 0.1677, 0.1592, 0.1888, 0.1487]])\n"
     ]
    }
   ],
   "source": [
    "d_k=keys.shape[-1]\n",
    "attn_weights_2=torch.nn.functional.softmax(attn_Score/d_k**2,dim=1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2861, 0.3929],\n",
      "        [0.2865, 0.3933],\n",
      "        [0.2865, 0.3931],\n",
      "        [0.2867, 0.3901],\n",
      "        [0.2867, 0.3880],\n",
      "        [0.2867, 0.3921]])\n"
     ]
    }
   ],
   "source": [
    "context_vector=attn_weights_2 @ values\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Create a class implementation of the Self Attention machanism with trainable weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttensionMachanism_v1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.w_query=nn.Parameter(torch.randn(d_in,d_out))\n",
    "        self.w_key=nn.Parameter(torch.randn(d_in,d_out))\n",
    "        self.w_value=nn.Parameter(torch.randn(d_in,d_out))\n",
    "\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        keys=inputs @ self.w_key\n",
    "        query=inputs @ self.w_query\n",
    "        values=inputs @ self.w_value\n",
    "\n",
    "        attn_score=query @ keys.T\n",
    "        attn_weights=torch.nn.functional.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "\n",
    "        context_vector=attn_weights @ values\n",
    "        return context_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1278, 0.2577],\n",
      "        [0.1365, 0.2556],\n",
      "        [0.1364, 0.2556],\n",
      "        [0.1294, 0.2624],\n",
      "        [0.1295, 0.2595],\n",
      "        [0.1310, 0.2618]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Am_v1=SelfAttensionMachanism_v1(d_in,d_out)\n",
    "context_vector=Am_v1(inputs)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttensionMachanism_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_value=nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        keys=self.w_key(inputs)\n",
    "        query=self.w_query(inputs)\n",
    "        values=self.w_value(inputs)\n",
    "\n",
    "        attn_score=query @ keys.T\n",
    "        attn_weights=torch.nn.functional.softmax(attn_score/keys.shape[-1]**0.5,dim=-1)\n",
    "\n",
    "        context_vector=attn_weights @ values\n",
    "        return context_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4496,  0.4982],\n",
      "        [-0.4462,  0.4957],\n",
      "        [-0.4462,  0.4956],\n",
      "        [-0.4478,  0.4971],\n",
      "        [-0.4468,  0.4963],\n",
      "        [-0.4479,  0.4971]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Am_v2=SelfAttensionMachanism_v2(d_in,d_out)\n",
    "context_vector=Am_v2(inputs)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal Self Attention Machanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1708, 0.1646, 0.1647, 0.1665, 0.1681, 0.1654],\n",
      "        [0.1723, 0.1579, 0.1587, 0.1690, 0.1818, 0.1603],\n",
      "        [0.1722, 0.1579, 0.1587, 0.1690, 0.1819, 0.1603],\n",
      "        [0.1696, 0.1616, 0.1621, 0.1681, 0.1755, 0.1630],\n",
      "        [0.1703, 0.1596, 0.1603, 0.1688, 0.1796, 0.1615],\n",
      "        [0.1701, 0.1617, 0.1622, 0.1680, 0.1750, 0.1631]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries=Am_v2.w_query(inputs)\n",
    "key=Am_v2.w_key(inputs)\n",
    "\n",
    "attention_score=queries @ key.T\n",
    "\n",
    "attention_weight=torch.nn.functional.softmax(attention_score/key.shape[-1]**0.5,dim=-1)\n",
    "print(attention_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length=attention_weight.shape[0]\n",
    "simple_mask=torch.tril(torch.ones(context_length,context_length))\n",
    "print(simple_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1708, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1723, 0.1579, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1722, 0.1579, 0.1587, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1696, 0.1616, 0.1621, 0.1681, 0.0000, 0.0000],\n",
      "        [0.1703, 0.1596, 0.1603, 0.1688, 0.1796, 0.0000],\n",
      "        [0.1701, 0.1617, 0.1622, 0.1680, 0.1750, 0.1631]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attention_weight * simple_mask\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5217, 0.4783, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3524, 0.3230, 0.3246, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2564, 0.2443, 0.2450, 0.2542, 0.0000, 0.0000],\n",
      "        [0.2031, 0.1904, 0.1911, 0.2013, 0.2141, 0.0000],\n",
      "        [0.1701, 0.1617, 0.1622, 0.1680, 0.1750, 0.1631]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rows_sum=masked_simple.sum(dim=1,keepdim=True)\n",
    "masked_attention_weights=masked_simple/rows_sum\n",
    "print(masked_attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1708,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1723, 0.1579,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1722, 0.1579, 0.1587,   -inf,   -inf,   -inf],\n",
      "        [0.1696, 0.1616, 0.1621, 0.1681,   -inf,   -inf],\n",
      "        [0.1703, 0.1596, 0.1603, 0.1688, 0.1796,   -inf],\n",
      "        [0.1701, 0.1617, 0.1622, 0.1680, 0.1750, 0.1631]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "masked = attention_weight.masked_fill(mask.bool(),-torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5025, 0.4975, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3355, 0.3321, 0.3323, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2508, 0.2493, 0.2494, 0.2505, 0.0000, 0.0000],\n",
      "        [0.2004, 0.1989, 0.1989, 0.2002, 0.2017, 0.0000],\n",
      "        [0.1671, 0.1661, 0.1661, 0.1668, 0.1676, 0.1662]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weight=torch.nn.functional.softmax(masked/key.shape[-1]**0.5,dim=-1)\n",
    "print(attention_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout=torch.nn.Dropout(p=0.5)\n",
    "example=torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6647, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4987, 0.0000, 0.5010, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3977, 0.3979, 0.4003, 0.4034, 0.0000],\n",
      "        [0.3341, 0.3322, 0.0000, 0.0000, 0.3353, 0.3325]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dropout(attention_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.dropout=torch.nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        keys=self.w_key(inputs)\n",
    "        query=self.w_query(inputs)\n",
    "        values=self.w_value(inputs)\n",
    "\n",
    "        attention_score=query @ keys.T\n",
    "        attention_weight=torch.nn.functional.softmax(attention_score/key.shape[-1]**0.5,dim=-1)\n",
    "        context_length=attention_weight.shape[0]\n",
    "        simple_mask=torch.tril(torch.ones(context_length,context_length))\n",
    "        masked_simple = attention_weight * simple_mask\n",
    "        rows_sum=masked_simple.sum(dim=1,keepdim=True)\n",
    "        masked_attention_weights=masked_simple/rows_sum\n",
    "        masked_attention_weights*=values\n",
    "        mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "        masked = attention_weight.masked_fill(mask.bool(),-torch.inf)\n",
    "        attention_weight=torch.nn.functional.softmax(masked/key.shape[-1]**0.5,dim=-1)\n",
    "        \n",
    "        \n",
    "\n",
    "        return self.dropout(attention_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5600, 0.5511, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3742, 0.3682, 0.3687, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2781, 0.2762, 0.2764, 0.2805, 0.0000, 0.0000],\n",
      "        [0.2229, 0.2194, 0.2196, 0.2229, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1832, 0.1833, 0.1870, 0.1882, 0.1852]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Am_v2=CausalSelfAttention(3,6)\n",
    "context_vector=Am_v2(inputs)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.stack((inputs,inputs,inputs,inputs,inputs,inputs,inputs,inputs),dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal Self Attention with the batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalSelfAttention_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.dropout=torch.nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        b,num_tokens,d_in=inputs.shape      \n",
    "        keys=self.w_key(inputs)\n",
    "        query=self.w_query(inputs)\n",
    "        values=self.w_value(inputs)\n",
    "\n",
    "        attention_score=query @ keys.transpose(1,2)\n",
    "        attn_Score.masked_fill_(self.mask.bool()[:num_tokens,:num_tokens].bool(),-torch.inf)\n",
    "        \n",
    "        attention_weight=torch.nn.functional.softmax(attention_score/key.shape[-1]**0.5,dim=-1)\n",
    "        attention_weight=self.dropout(attention_weight)\n",
    "        context_vector=attention_weight @values\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.5337, -0.1051],\n",
      "         [-0.5323, -0.1080],\n",
      "         [-0.5323, -0.1079],\n",
      "         [-0.5297, -0.1076],\n",
      "         [-0.5311, -0.1066],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length=batch.shape[1]\n",
    "ca=CausalSelfAttention_v2(d_in,d_out,context_length,0.0)\n",
    "context_vector=ca(batch)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multihead attention Machanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttensionMachanism(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bais=False):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([\n",
    "             CausalSelfAttention_v2(d_in,d_out,context_length,dropout,qkv_bais )\n",
    "             for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x ):\n",
    "        return torch.cat([head(x) for head in self.heads],dim=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5191, -0.0904,  0.5650,  0.3897,  0.4649,  0.2185],\n",
      "         [-0.5915, -0.1200,  0.4928,  0.3737,  0.5907,  0.2765],\n",
      "         [-0.5915, -0.1199,  0.4938,  0.3240,  0.5176,  0.2352],\n",
      "         [-0.3656, -0.0533,  0.5638,  0.3856,  0.5933,  0.2779],\n",
      "         [-0.4538, -0.0808,  0.4739,  0.3267,  0.4188,  0.2054],\n",
      "         [-0.5888, -0.1201,  0.5641,  0.3882,  0.5037,  0.2243]],\n",
      "\n",
      "        [[-0.5930, -0.1168,  0.4707,  0.3309,  0.5913,  0.2768],\n",
      "         [-0.5915, -0.1200,  0.5649,  0.3898,  0.3950,  0.1895],\n",
      "         [-0.5233, -0.0920,  0.5649,  0.3896,  0.5907,  0.2765],\n",
      "         [-0.5886, -0.1195,  0.5638,  0.3856,  0.5933,  0.2779],\n",
      "         [-0.4538, -0.0808,  0.3526,  0.2706,  0.5923,  0.2775],\n",
      "         [-0.5888, -0.1201,  0.5641,  0.3882,  0.4634,  0.2187]],\n",
      "\n",
      "        [[-0.5930, -0.1168,  0.5650,  0.3897,  0.4988,  0.2215],\n",
      "         [-0.5915, -0.1200,  0.5649,  0.3898,  0.4654,  0.2187],\n",
      "         [-0.5233, -0.0920,  0.4708,  0.3309,  0.5907,  0.2765],\n",
      "         [-0.4182, -0.1309,  0.5638,  0.3856,  0.5933,  0.2779],\n",
      "         [-0.5901, -0.1184,  0.4797,  0.3641,  0.5923,  0.2775],\n",
      "         [-0.5888, -0.1201,  0.5641,  0.3882,  0.3924,  0.1847]],\n",
      "\n",
      "        [[-0.5930, -0.1168,  0.5650,  0.3897,  0.4169,  0.2046],\n",
      "         [-0.5915, -0.1200,  0.4345,  0.2944,  0.5907,  0.2765],\n",
      "         [-0.5915, -0.1199,  0.3637,  0.2283,  0.4654,  0.2187],\n",
      "         [-0.5064, -0.1598,  0.5638,  0.3856,  0.5221,  0.2446],\n",
      "         [-0.5901, -0.1184,  0.5640,  0.3829,  0.4933,  0.2475],\n",
      "         [-0.5888, -0.1201,  0.5641,  0.3882,  0.5930,  0.2777]],\n",
      "\n",
      "        [[-0.4536, -0.0782,  0.5650,  0.3897,  0.5913,  0.2768],\n",
      "         [-0.4275, -0.0639,  0.5649,  0.3898,  0.3946,  0.1856],\n",
      "         [-0.5915, -0.1199,  0.5649,  0.3896,  0.5907,  0.2765],\n",
      "         [-0.5886, -0.1195,  0.4847,  0.3680,  0.4635,  0.2180],\n",
      "         [-0.4538, -0.0808,  0.4369,  0.2899,  0.3359,  0.1599],\n",
      "         [-0.5888, -0.1201,  0.5641,  0.3882,  0.5037,  0.2243]],\n",
      "\n",
      "        [[-0.3874, -0.0515,  0.3316,  0.2567,  0.5913,  0.2768],\n",
      "         [-0.5915, -0.1200,  0.5649,  0.3898,  0.5907,  0.2765],\n",
      "         [-0.5915, -0.1199,  0.4346,  0.2943,  0.5907,  0.2765],\n",
      "         [-0.5187, -0.0910,  0.5638,  0.3856,  0.5933,  0.2779],\n",
      "         [-0.5901, -0.1184,  0.5640,  0.3829,  0.4188,  0.2054],\n",
      "         [-0.5888, -0.1201,  0.5641,  0.3882,  0.5037,  0.2243]],\n",
      "\n",
      "        [[-0.5930, -0.1168,  0.5650,  0.3897,  0.5913,  0.2768],\n",
      "         [-0.5915, -0.1200,  0.5649,  0.3898,  0.3918,  0.1779],\n",
      "         [-0.5915, -0.1199,  0.4346,  0.2943,  0.5907,  0.2765],\n",
      "         [-0.5886, -0.1195,  0.4925,  0.3198,  0.5221,  0.2446],\n",
      "         [-0.3849, -0.0529,  0.5640,  0.3829,  0.5214,  0.2443],\n",
      "         [-0.5888, -0.1201,  0.3633,  0.2271,  0.4634,  0.2187]],\n",
      "\n",
      "        [[-0.5191, -0.0904,  0.5650,  0.3897,  0.4988,  0.2215],\n",
      "         [-0.5232, -0.0921,  0.4348,  0.2941,  0.5907,  0.2765],\n",
      "         [-0.5915, -0.1199,  0.5649,  0.3896,  0.5907,  0.2765],\n",
      "         [-0.5886, -0.1195,  0.5638,  0.3856,  0.5933,  0.2779],\n",
      "         [-0.4296, -0.1322,  0.4369,  0.2899,  0.5923,  0.2775],\n",
      "         [-0.3832, -0.0542,  0.5641,  0.3882,  0.4056,  0.1946]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "m1=MultiheadAttensionMachanism(d_in,d_out,context_length,0.1,3,False)\n",
    "context_vector=m1(batch)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Multihead Attention with Weight Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttensionMachanism_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bais=False):\n",
    "        super().__init__()\n",
    "        assert( d_out%num_heads==0),\\\n",
    "            \"d_out must be Divisible by numeber of Heads\"\n",
    "        self.d_out=d_out\n",
    "        self.d_in=d_in\n",
    "        self.num_head=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkv_bais)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkv_bais)\n",
    "        self.w_value=nn.Linear(d_in,d_out,bias=qkv_bais)\n",
    "        self.out_proj=nn.Linear(d_out,d_out)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in=x.shape\n",
    "\n",
    "        keys=self.w_key(x)\n",
    "        queries=self.w_query(x)\n",
    "        value=self.w_value(x)\n",
    "\n",
    "        keys=keys.view(b,num_tokens,self.num_head,self.head_dim)\n",
    "        queries=queries.view(b,num_tokens,self.num_head,self.head_dim)\n",
    "        values=value.view(b,num_tokens,self.num_head,self.head_dim)\n",
    "\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "\n",
    "        attn_scores=queries @ keys.transpose(2,3)\n",
    "        \n",
    "        mask_bool=self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores=masked_fill_(mask_bool,-torch.inf)\n",
    "\n",
    "        attn_weights=torch.softmax(attn_scores/key.shape[-1]**0.5,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "\n",
    "        context_vec=(attn_weights @ value).transpose(1,2)\n",
    "\n",
    "        context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec=self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttensionMachanism_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bais=False):\n",
    "        super().__init__()\n",
    "        assert( d_out%num_heads==0),\\\n",
    "            \"d_out must be Divisible by numeber of Heads\"\n",
    "        self.d_out=d_out\n",
    "        self.d_in=d_in\n",
    "        self.num_head=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkv_bais)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkv_bais)\n",
    "        self.w_value=nn.Linear(d_in,d_out,bias=qkv_bais)\n",
    "        self.out_proj=nn.Linear(d_out,d_out)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,num_tokens,d_in=x.shape\n",
    "\n",
    "        keys=self.w_key(x)\n",
    "        queries=self.w_query(x)\n",
    "        value=self.w_value(x)\n",
    "\n",
    "        keys=keys.view(b,num_tokens,self.num_head,self.head_dim)\n",
    "        queries=queries.view(b,num_tokens,self.num_head,self.head_dim)\n",
    "        values=value.view(b,num_tokens,self.num_head,self.head_dim)\n",
    "\n",
    "        keys=keys.transpose(1,2)\n",
    "        queries=queries.transpose(1,2)\n",
    "        values=values.transpose(1,2)\n",
    "\n",
    "        attn_scores=queries @ keys.transpose(2,3)\n",
    "        \n",
    "        mask_bool=self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores=masked_fill_(mask_bool,-torch.inf)\n",
    "\n",
    "        attn_weights=torch.softmax(attn_scores/key.shape[-1]**0.5,dim=-1)\n",
    "        attn_weights=self.dropout(attn_weights)\n",
    "\n",
    "        context_vec=(attn_weights @ value).transpose(1,2)\n",
    "\n",
    "        context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
    "        context_vec=self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Architecture Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Tokenize All the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Apply the Layer Normalization on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example=torch.randn(2,5)\n",
    "layer=nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out=layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the Inputs is:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance of the inputs is:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean=out.mean(dim=-1,keepdim=True)\n",
    "var=out.var(dim=-1,keepdim=True)\n",
    "print(\"Mean of the Inputs is:\\n\",mean)\n",
    "print(\"Variance of the inputs is:\\n\",var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Values:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean of Normalized values:\n",
      " tensor([[9.9341e-09],\n",
      "        [5.9605e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance of Normalized Values:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm=(out-mean)/torch.sqrt(var)\n",
    "mean=out_norm.mean(dim=-1,keepdim=True)\n",
    "var=out_norm.var(dim=-1,keepdim=True)\n",
    "print(\"Normalized Values:\\n\",out_norm)\n",
    "print(\"Mean of Normalized values:\\n\",mean)\n",
    "print(\"Variance of Normalized Values:\\n\",var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Values:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean of Normalized values:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance of Normalized Values:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "\n",
    "print(\"Normalized Values:\\n\",out_norm)\n",
    "print(\"Mean of Normalized values:\\n\",mean)\n",
    "print(\"Variance of Normalized Values:\\n\",var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the class of this code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var  = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShortCut Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "layer_sizes, use_shortcut=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) \n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entire GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        ) \n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.4708,  0.5737, -0.5967,  ...,  0.2019, -0.5665,  0.1800],\n",
      "         [-0.3895, -0.1978, -0.8885,  ...,  0.2242, -1.2341,  0.1752],\n",
      "         [ 0.6973, -0.3432, -0.6080,  ...,  0.3747, -0.6967,  0.1088],\n",
      "         [-0.2962, -0.6957, -1.1371,  ...,  0.3579,  0.3058, -0.2915]],\n",
      "\n",
      "        [[-0.1514,  0.3329, -0.9740,  ..., -0.1368, -0.6974, -0.1851],\n",
      "         [-0.4894, -0.3492, -0.9759,  ...,  0.2951, -0.3396,  0.2109],\n",
      "         [ 0.5082, -0.1425,  0.2549,  ...,  0.1618,  0.1304, -0.3092],\n",
      "         [-0.4146, -0.0514, -0.5187,  ..., -0.1869, -0.1303, -0.4969]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number Of Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 123,822,336\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory Requirement for this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 619.58 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating The New Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 13240, 11381,  4307,  7640, 16620, 34991]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Laur inhab DistrinetalkQueue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE haven't Trained the Model Yet, Becuase of Which it will produce the Dummy or False Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0001,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0001,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's Use the Real Dataset so We will be using a book data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Data Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTversion1(Dataset):\n",
    "    def __init__(self,text#text file to be read\n",
    "                 ,tokenizer #byte pair tokenizer\n",
    "                 ,max_length #cotext size\n",
    "                 ,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "        \n",
    "        token_ids=tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.input_ids[i],self.target_ids[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dataset loader\n",
    "def create_dataset_Loader_V1(text,batch_size=4#number of CPU cores to use\n",
    "                         ,max_length=256,\n",
    "                         stride=128,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True,\n",
    "                         num_workers=0#number of CPU threads that we can use simultaneously\n",
    "                         ):\n",
    "    #initialize the tokenizer\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset=GPTversion1(text,tokenizer,max_length,stride)\n",
    "    dataLoader=DataLoader(dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
    "    return dataLoader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataset_Loader_V1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataset_Loader_V1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4630.5\n"
     ]
    }
   ],
   "source": [
    "print(total_tokens * (train_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")  \n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Step 1: Initialize lists to track losses and tokens seen\n",
    "\n",
    "Step 2: Start the main training loop\n",
    "\n",
    "Step 3: Reset loss gradients from previous batch iteration\n",
    "\n",
    "Step 4: Calculate loss gradients\n",
    "\n",
    "Step 5: Update model weights using loss gradients\n",
    "\n",
    "Step 6: Optional evaluation step\n",
    "\n",
    "Step 7: Print a sample text after each epoch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
      "Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n",
      "Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n",
      "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
      "Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n",
      "Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n",
      "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n",
      "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
      "Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n",
      "Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n",
      "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
      "Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n",
      "Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
      "Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n",
      "Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n",
      "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
      "Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n",
      "Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n",
      "Training completed in 7.48 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature Scalling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toward\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABNiElEQVR4nO3dd1gUV/s38O9Sl0UBka7UYAFBpSSKRsESiLHEmJ/ErgiWmICIFY2KBUuiiF2s2GLUaEj04VExiYqxREEskaAICFEIARVQAsjuef/gZR7XZXGpM+D9ua694p49M/td3HgzM2fOETHGGAghhBAiSGp8ByCEEEKIclSoCSGEEAGjQk0IIYQIGBVqQgghRMCoUBNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBEyD7wCNTSaT4fHjx2jZsiVEIhHfcQghhLyFGGMoKiqChYUF1NSqP2Z+6wr148ePYWlpyXcMQgghBFlZWWjbtm21fd66Qt2yZUsAFT8cPT09ntMQQgh5GxUWFsLS0pKrSdV56wp15eluPT09KtSEEEJ4pcolWBpMRgghhAgYr4X6woULGDx4MCwsLCASiRATE/PGbc6fPw83NzeIxWLY2dlh27ZtDR+UEEII4QmvhfrFixfo0qULNm3apFL/9PR0fPTRR+jVqxdu3LiB+fPnIygoCMeOHWvgpIQQQgg/eL1GPWDAAAwYMEDl/tu2bYOVlRUiIyMBAA4ODrh+/TrWrFmDTz/9tIFSEkIam1QqxcuXL/mOQUitaWpqQl1dvV721aQGk12+fBne3t5ybT4+Pti1axdevnwJTU1NhW1KS0tRWlrKPS8sLGzwnISQ2mGMIScnB8+ePeM7CiF1ZmBgADMzszrP2dGkCnVOTg5MTU3l2kxNTVFeXo68vDyYm5srbLNy5UosWbKksSISQuqgskibmJhAIpHQpESkSWKMobi4GLm5uQBQZW2qiSZVqAHFoeyMsSrbK4WGhiIkJIR7XnnvGiFEWKRSKVekW7duzXccQupER0cHAJCbmwsTE5M6nQZvUoXazMwMOTk5cm25ubnQ0NBQ+j+2trY2tLW1GyMeIaoL06/mtYLGyyEgldekJRIJz0kIqR+V3+WXL1/WqVA3qfuoPTw8EBcXJ9d25swZuLu7V3l9mhDS9NDpbtJc1Nd3mddC/fz5cyQlJSEpKQlAxe1XSUlJyMzMBFBx2nrcuHFc/6lTp+Lhw4cICQlBcnIydu/ejV27dmHWrFl8xCeEEEIaHK+nvq9fv44+ffpwzyuvJY8fPx7R0dHIzs7mijYA2NraIjY2FjNmzMDmzZthYWGBDRs20K1ZhBBCmi1eC7WXlxc3GKwq0dHRCm2enp5ITExswFSEEKGxmfefRn2/jFUDVe77ptOblQcezYmXlxe6du3KzWnRFG3fvh3ffvstEhMTUVRUhKdPn8LAwIDvWFVqUoPJCCFEaLKzs7k/Hz58GIsWLUJKSgrXVjn6tylQNh9Fc3m/VxUXF+PDDz/Ehx9+iNDQUF4yqKpJDSYjhBChMTMz4x76+voQiURybRcuXJBbn2DJkiUoLy/ntheJRIiKisKgQYMgkUjg4OCAy5cvIzU1FV5eXtDV1YWHhwcePHjAbRMWFoauXbsiKioKlpaWkEgkGD58uMJEMXv27IGDgwPEYjE6duyILVu2cK9lZGRAJBLhyJEj8PLyglgsxoEDB5Cfn4+RI0eibdu2kEgkcHZ2xqFDh7jtJkyYgPPnz2P9+vUQiUQQiUTIyMhAdHS0whFpTEyM3BmHyty7d++GnZ0dtLW1wRhDQUEBJk+eDBMTE+jp6aFv3764efNmPf0NVS04OBjz5s1D9+7dG/R96gMVakIIaSCnT5/GmDFjEBQUhLt37yIqKgrR0dEIDw+X67ds2TKMGzcOSUlJ6NixI0aNGoUpU6YgNDQU169fBwB8+eWXctukpqbiyJEjOHHiBE6dOoWkpCR88cUX3Os7duzAggULEB4ejuTkZKxYsQILFy7E3r175fYzd+5cBAUFITk5GT4+PigpKYGbmxtOnjyJO3fuYPLkyRg7diyuXr0KAFi/fj08PDwwadIkZGdnIzs7u0ZzU1TmPnbsGDeQeODAgcjJyUFsbCwSEhLg6uqKfv364cmTJ0r306lTJ7Ro0ULpo1OnTipnEjo69U0IIQ0kPDwc8+bNw/jx4wEAdnZ2WLZsGebMmYPFixdz/fz8/ODr6wugonB6eHhg4cKF8PHxAQBMnz4dfn5+cvsuKSnB3r170bZtWwDAxo0bMXDgQKxduxZmZmZYtmwZ1q5di2HDhgGoGIxb+ctCZR6g4siysk+lV++kCQwMxKlTp3D06FF069YN+vr60NLSgkQigZmZWY1/JmVlZdi/fz+MjY0BAL/88gtu376N3Nxcbs6LNWvWICYmBt9//z0mT55c5X5iY2OrnQ++Od2yS4WaEEIaSEJCAq5duyZ3BC2VSlFSUoLi4mJuQozOnTtzr1dOk+zs7CzXVlJSgsLCQujp6QEArKysuCINVMwzIZPJkJKSAnV1dWRlZcHf3x+TJk3i+pSXl0NfX36yHXd3d7nnUqkUq1atwuHDh/Ho0SNuvQRdXd26/jgAANbW1lyRBip+Rs+fP1eYtOrff/+VO91f1X7eFlSoCSGkgchkMixZskThiBUAxGIx9+dXj/4qr+lW1SaTyZS+V2UfkUjE9duxYwe6desm1+/1GbJeL8Br167FunXrEBkZCWdnZ+jq6iI4OBhlZWXKPygANTU1hbt4qjriff39ZDIZzM3Nce7cOYW+1Y3C7tSpEx4+fKj0dWtra/zxxx/VZm4qqFATQkgDcXV1RUpKCuzt7et935mZmXj8+DEsLCwAVKwuqKamhvbt28PU1BRt2rRBWloaRo8eXaP9xsfH4+OPP8aYMWMAVBTS+/fvw8HBgeujpaUFqVQqt52xsTGKiorw4sULrhhXXoOujqurK3JycqChoQEbGxuVc9Kpb0IIIXW2aNEiDBo0CJaWlhg+fDjU1NRw69Yt3L59G8uXL6/TvsViMcaPH481a9agsLAQQUFB8PX15a4bh4WFISgoCHp6ehgwYABKS0tx/fp1PH36VG6hotfZ29vj2LFjuHTpElq1aoWIiAjk5OTIFWobGxtcvXoVGRkZaNGiBQwNDdGtWzdIJBLMnz8fgYGB+P3331W6f7x///7w8PDA0KFDsXr1anTo0AGPHz9GbGwshg4dqnBqvlJdT33n5OQgJycHqampAIDbt2+jZcuWsLKygqGhYZ32Xd9o1DchhDQQHx8fnDx5EnFxcXj33XfRvXt3RERE1Mv1VXt7ewwbNgwfffQRvL294eTkJHf7VUBAAHbu3Ino6Gg4OzvD09MT0dHRsLW1rXa/CxcuhKurK3x8fODl5QUzMzMMHTpUrs+sWbOgrq4OR0dHGBsbIzMzE4aGhjhw4ABiY2O5W7rCwsLe+DlEIhFiY2PRu3dvTJw4Ee3bt8eIESOQkZGhsKxxfdq2bRtcXFy4a/i9e/eGi4sLfvrppwZ7z9oSseqmBmuGCgsLoa+vj4KCAm5QBiGNjlbPUlBSUoL09HTY2trKXb8lisLCwhATE6PSqWXCn+q+0zWpRXRETQghhAgYFWpCCCFEwKhQE0JIExMWFkanvd8iVKgJIYQQAaNCTQghhAgYFWpCCCFEwKhQE0IIIQJGhZoQQggRMCrUhBBCiIBRoSaEkDoQiUTVPiZMmMB3xHrn5eWF4OBgvmPUSWlpKQIDA2FkZARdXV0MGTIEf/31V7XbXLhwAYMHD4aFhQVEIhFiYmIaJSstykEIEb7qplxtkPdTfRrX7Oxs7s+HDx/GokWLkJKSwrXp6OjUa7SG9PLly0Zddaqx3+9VwcHBOHHiBL777ju0bt0aM2fOxKBBg5CQkKCwFGilFy9eoEuXLvDz88Onn37aaFnpiJoQQurAzMyMe+jr60MkEsm1XbhwAW5ubhCLxbCzs8OSJUtQXl7ObS8SiRAVFYVBgwZBIpHAwcEBly9fRmpqKry8vKCrqwsPDw88ePCA2yYsLAxdu3ZFVFQULC0tIZFIMHz4cDx79kwu2549e+Dg4ACxWIyOHTvKLdqRkZEBkUiEI0eOwMvLC2KxGAcOHEB+fj5GjhyJtm3bQiKRcAtsVJowYQLOnz+P9evXc2cNMjIyEB0drbB+dExMDLdO9qu5d+/eDTs7O2hra4MxhoKCAkyePBkmJibQ09ND3759cfPmzXr6G1JUUFCAXbt2Ye3atejfvz9cXFxw4MAB3L59G2fPnlW63YABA7B8+fIq1xdvSFSoCSGkgZw+fRpjxoxBUFAQ7t69i6ioKERHRyM8PFyu37JlyzBu3DgkJSWhY8eOGDVqFKZMmYLQ0FBcv34dAPDll1/KbZOamoojR47gxIkTOHXqFJKSkvDFF19wr+/YsQMLFixAeHg4kpOTsWLFCixcuBB79+6V28/cuXMRFBSE5ORk+Pj4oKSkBG5ubjh58iTu3LmDyZMnY+zYsbh69SoAYP369fDw8MCkSZOQnZ2N7OxsWFpaqvwzqcx97Ngxbna1gQMHIicnB7GxsUhISICrqyv69euHJ0+eKN1Pp06d0KJFC6WPTp06Kd02ISEBL1++hLe3N9dmYWEBJycnXLp0SeXP0ljo1DchhDSQ8PBwzJs3D+PHjwcA2NnZYdmyZZgzZw4WL17M9fPz84Ovry+AisLp4eGBhQsXwsfHBwAwffp0+Pn5ye27pKQEe/fuRdu2bQEAGzduxMCBA7F27VqYmZlh2bJlWLt2LXf0Z2try/2yUJkHqDgF/PoR4qxZs7g/BwYG4tSpUzh69Ci6desGfX19aGlpQSKRcGtf10RZWRn2798PY2NjAMAvv/yC27dvIzc3F9ra2gCANWvWICYmBt9//z0mT55c5X5iY2Px8uVLpe9T3Sn1nJwcaGlpoVWrVnLtpqamyMnJqelHanBUqAkhpIEkJCTg2rVrckfQUqkUJSUlKC4uhkQiAQB07tyZe71yDWZnZ2e5tpKSEhQWFnJLIlpZWXFFGgA8PDwgk8mQkpICdXV1ZGVlwd/fn1tvGQDKy8uhry9/vd/d3V3uuVQqxapVq3D48GE8evQIpaWlKC0tha6ubl1/HAAAa2trrkgDFT+j58+fo3Xr1nL9/v33X7nT/VXtp74xxuRO1QsFFWpCCGkgMpkMS5YsqfKa5qvrE7969FdZKKpqk8lkSt+rso9IJOL67dixA926dZPr9/pAqdcL8Nq1a7Fu3TpERkbC2dkZurq6CA4ORllZmfIPCkBNTQ2MMbm2qo54X38/mUwGc3NznDt3TqHv69e8X9WpUyc8fPhQ6evW1tb4448/qnzNzMwMZWVlePr0qdxRdW5uLnr06KF0n3yhQk0IIQ3E1dUVKSkpsLe3r/d9Z2Zm4vHjx7CwsAAAXL58GWpqamjfvj1MTU3Rpk0bpKWlYfTo0TXab3x8PD7++GOMGTMGQEUhvX//PhwcHLg+WlpakEqlctsZGxujqKgIL1684IqxKit8ubq6IicnBxoaGrCxsVE5Z11Ofbu5uUFTUxNxcXHcJYfs7GzcuXMHX3/9tcoZGgsVakIIaSCLFi3CoEGDYGlpieHDh0NNTQ23bt3C7du3sXz58jrtWywWY/z48VizZg0KCwsRFBQEX19f7rpxWFgYgoKCoKenhwEDBqC0tBTXr1/H06dPERISonS/9vb2OHbsGC5duoRWrVohIiICOTk5coXaxsYGV69eRUZGBlq0aAFDQ0N069YNEokE8+fPR2BgIH7//XdER0e/8XP0798fHh4eGDp0KFavXo0OHTrg8ePHiI2NxdChQxVOzVeqy6lvfX19+Pv7Y+bMmWjdujUMDQ0xa9YsODs7o3///ly/fv364ZNPPuEG8j1//hypqanc6+np6UhKSoKhoSGsrKxqnedNeB/1vWXLFtja2kIsFsPNzQ3x8fHV9j948CC6dOkCiUQCc3Nz+Pn5IT8/v5HSEkKI6nx8fHDy5EnExcXh3XffRffu3REREVEv11ft7e0xbNgwfPTRR/D29oaTk5Pc7VcBAQHYuXMnoqOj4ezsDE9PT0RHR8PW1rba/S5cuBCurq7w8fGBl5cXzMzMMHToULk+s2bNgrq6OhwdHWFsbIzMzEwYGhriwIEDiI2N5W7pCgsLe+PnEIlEiI2NRe/evTFx4kS0b98eI0aMQEZGBne9viGsW7cOQ4cOha+vL3r27AmJRIITJ07IXRp48OAB8vLyuOfXr1+Hi4sLXFxcAAAhISFwcXHBokWLGiwnAIjY6xcVGtHhw4cxduxYbNmyBT179kRUVBR27tyJu3fvVvnbycWLF+Hp6Yl169Zh8ODBePToEaZOnYp27drhhx9+UOk9CwsLoa+vj4KCAm5QBiGNrroJPGow2UZzUlJSgvT0dO4Xd6JcWFgYYmJiVDq1TPhT3Xe6JrWI1yPqiIgI+Pv7IyAgAA4ODoiMjISlpSW2bt1aZf8rV67AxsYGQUFBsLW1xfvvv48pU6Zw9xkSQgghzQ1vhbqsrAwJCQlyN5wDgLe3t9Ibznv06IG//voLsbGxYIzh77//xvfff4+BAwc2RmRCCCGk0fFWqPPy8iCVShWuQVR3w3mPHj1w8OBBfPbZZ9DS0oKZmRkMDAywceNGpe9TWlqKwsJCuQchhDRlYWFhdNr7LcL7YLLXby6v7obzu3fvIigoCIsWLUJCQgJOnTqF9PR0TJ06Ven+V65cCX19fe5Rk6nuCCGEEL7xVqiNjIygrq6ucPScm5urdKTfypUr0bNnT8yePRudO3eGj48PtmzZgt27d8utYPOq0NBQFBQUcI+srKx6/yyEEEJIQ+GtUGtpacHNzQ1xcXFy7XFxcUpnhikuLoaamnzkyqH0ygava2trQ09PT+5BCCGENBW8nvoOCQnBzp07sXv3biQnJ2PGjBnIzMzkTmWHhoZi3LhxXP/Bgwfj+PHj2Lp1K9LS0vDbb78hKCgI7733Hjc7DyGEENKc8Doz2WeffYb8/HwsXboU2dnZcHJyQmxsLDcZQHZ2NjIzM7n+EyZMQFFRETZt2oSZM2fCwMAAffv2xerVq/n6CIQQQkiD4nXCEz7QhCdEEGjCEwU04QlpbprFhCeEEEIIqR4VakIIqQORSFTtY8KECXxHrHdeXl4IDg7mO0adeHl5KfxdjRgxgu9YVaLVswghgue817lR3+/2+Nsq93311tDDhw9j0aJFSElJ4dp0dHTqNVtDevnyZbXLQzb193vdpEmTsHTpUu65UP+u6IiaEELqwMzMjHvo6+tDJBLJtV24cAFubm4Qi8Wws7PDkiVLUF5ezm0vEokQFRWFQYMGQSKRwMHBAZcvX0Zqaiq8vLygq6sLDw8PPHjwgNsmLCwMXbt2RVRUFCwtLSGRSDB8+HA8e/ZMLtuePXvg4OAAsViMjh07yq2ulZGRAZFIhCNHjsDLywtisRgHDhxAfn4+Ro4cibZt20IikXArYVWaMGECzp8/j/Xr13NHohkZGYiOjoaBgYHc+8fExMhNYFWZe/fu3bCzs4O2tjYYYygoKMDkyZNhYmICPT099O3bFzdv3qynvyHlJBKJwt+fEFGhJoSQBnL69GmMGTMGQUFBuHv3LqKiohAdHY3w8HC5fsuWLcO4ceOQlJSEjh07YtSoUZgyZQpCQ0O5RYcq10SulJqaiiNHjuDEiRM4deoUkpKS8MUXX3Cv79ixAwsWLEB4eDiSk5OxYsUKLFy4EHv37pXbz9y5cxEUFITk5GT4+PigpKQEbm5uOHnyJO7cuYPJkydj7NixuHr1KgBg/fr18PDwwKRJk5CdnY3s7OwazfhYmfvYsWPcNKgDBw5ETk4OYmNjkZCQAFdXV/Tr1w9PnjxRup9OnTqhRYsWSh+dOnV6Y5aDBw/CyMgInTp1wqxZs1BUVKTy52hMdOqbEEIaSHh4OObNm4fx48cDAOzs7LBs2TLMmTMHixcv5vr5+fnB19cXQEXh9PDwwMKFC+Hj4wMAmD59Ovz8/OT2XVJSgr1796Jt27YAgI0bN2LgwIFYu3YtzMzMsGzZMqxduxbDhg0DANja2nK/LFTmAYDg4GCuT6VZs2Zxfw4MDMSpU6dw9OhRdOvWDfr6+tDS0uKORmuqrKwM+/fvh7GxMQDgl19+we3bt5GbmwttbW0AwJo1axATE4Pvv/8ekydPrnI/sbGxePnypdL3edMp9dGjR8PW1hZmZma4c+cOQkNDcfPmTYVJuISACjUhhDSQhIQEXLt2Te4IWiqVoqSkBMXFxZBIJACAzp07c69XTqHs7Ows11ZSUoLCwkLuVh4rKyuuSAOAh4cHZDIZUlJSoK6ujqysLPj7+2PSpElcn/LycoXTu+7u7nLPpVIpVq1ahcOHD+PRo0coLS1FaWkpdHV16/rjAABYW1tzRRqo+Bk9f/4crVu3luv377//yp3ur2o/dfHqz8XJyQnt2rWDu7s7EhMT4erqWqd91zcq1IQQ0kBkMhmWLFmicMQKQO6+2leP/iqv6VbVJpPJlL5XZR+RSMT127FjB7p16ybXr3La5UqvF+C1a9di3bp1iIyMhLOzM3R1dREcHIyysjLlHxSAmpqawlTOVR3xvv5+MpkM5ubmOHfunELf1695v6pTp054+PCh0tetra3xxx9/VJv5Va6urtDU1MT9+/epUBNCyNvC1dUVKSkpsLe3r/d9Z2Zm4vHjx9z0yZcvX4aamhrat28PU1NTtGnTBmlpaRg9enSN9hsfH4+PP/4YY8aMAVBRSO/fvw8HBweuj5aWFqRSqdx2xsbGKCoqwosXL7hirMpSnK6ursjJyYGGhgZsbGxUzlnXU9+v++OPP/Dy5UuYm5vXaLvGQIWaEEIayKJFizBo0CBYWlpi+PDhUFNTw61bt3D79m0sX768TvsWi8UYP3481qxZg8LCQgQFBcHX15e7bhwWFoagoCDo6elhwIABKC0txfXr1/H06VOEhIQo3a+9vT2OHTuGS5cuoVWrVoiIiEBOTo5cobaxscHVq1eRkZGBFi1awNDQEN26dYNEIsH8+fMRGBiI33//HdHR0W/8HP3794eHhweGDh2K1atXo0OHDnj8+DFiY2MxdOhQhVPzlepy6vvBgwc4ePAgPvroIxgZGeHu3buYOXMmXFxc0LNnz1rvt6HQqG9CCGkgPj4+OHnyJOLi4vDuu++ie/fuiIiIqPP1VaCioA4bNgwfffQRvL294eTkJHf7VUBAAHbu3Ino6Gg4OzvD09MT0dHRsLW1rXa/CxcuhKurK3x8fODl5QUzMzMMHTpUrs+sWbOgrq4OR0dHGBsbIzMzE4aGhjhw4ABiY2O5W7rCwsLe+DlEIhFiY2PRu3dvTJw4Ee3bt8eIESOQkZGhdMnjutLS0sLPP/8MHx8fdOjQAUFBQfD29sbZs2cVLg0IAc31TQgfaK5vBTTXt+rCwsIQExOj0qllwh+a65sQQgh5C1ChJoQQQgSMCjUhhDQxYWFhdNr7LVKrQh0dHY3i4uL6zkIIIYSQ19SqUIeGhsLMzAz+/v64dOlSfWcihBBCyP9Xq0L9119/4cCBA3j69Cn69OmDjh07YvXq1cjJyanvfISQt8xbdiMKacbq67tcq0Ktrq6OIUOG4Pjx48jKysLkyZNx8OBBWFlZYciQIfjxxx+rneqOEEJeVzmTFF1WI81F5Xe5rmtu13lmMhMTE/Ts2RMpKSm4d+8ebt++jQkTJsDAwAB79uyBl5dXXd+CEPIWUFdXh4GBAXJzcwFUrBX86lrGhDQVjDEUFxcjNzcXBgYGdZ5EpdaF+u+//8b+/fuxZ88epKWlYejQoTh58iT69++Pf//9F1999RXGjx9f7aTphBDyqsrpLyuLNSFNmYGBQa2WAn1drWYmGzx4ME6fPo327dsjICAA48aNg6GhoVyfx48fo23btoI7BU4zkxFBoJnJqiWVSqtdcIEQodPU1Kz2SLomtahWR9QmJiY4f/48PDw8lPYxNzdHenp6bXZPCHnLqaurC3LOZUL4UKvBZJ6enlWu11lWVoZ9+/YBqJhovT4mnieEEELeZrUq1H5+figoUDw9V1RUBD8/vzqHIoQQQkiFWhVqxliVozH/+usv6OtXc+2NEEIIITVSo2vULi4uEIlEEIlE6NevHzQ0/re5VCpFeno6Pvzww3oPSQghhLytalSoKxcPT0pKgo+PD1q0aMG9pqWlBRsbG3z66af1GpAQQgh5m9WoUC9evBgAYGNjg88++4wWdyeEEEIaWK2uUY8fP77eivSWLVtga2sLsVgMNzc3xMfHV9u/tLQUCxYsgLW1NbS1tfHOO+9g9+7d9ZKFEEIIERqVj6gNDQ1x7949GBkZoVWrVtVO7ffkyROV9nn48GEEBwdjy5Yt6NmzJ6KiojBgwADcvXsXVlZWVW7j6+uLv//+G7t27YK9vT1yc3NRXl6u6scghBBCmhSVC/W6devQsmVL7s/1MQdvREQE/P39ERAQAACIjIzE6dOnsXXrVqxcuVKh/6lTp3D+/HmkpaVxM6HZ2NjUOQchhBAiVCoX6vHjx3N/njBhQp3fuKysDAkJCZg3b55cu7e3t9I1rn/66Se4u7vj66+/xv79+6Grq4shQ4Zg2bJl0NHRqXKb0tJSlJaWcs8LCwvrnJ0QQghpLCoX6poUOFXm0M7Ly4NUKoWpqalcu6mpqdJ1rdPS0nDx4kWIxWL88MMPyMvLw7Rp0/DkyROl16lXrlyJJUuWqJydEEIIERKVC7WBgcEbT3dXToQilUpVDvD6PpVNpgIAMpkMIpEIBw8e5CZWiYiIwP/93/9h8+bNVR5Vh4aGIiQkhHteWFgIS0tLlfMRQgghfFK5UP/666/1+sZGRkZQV1dXOHrOzc1VOMquZG5ujjZt2sjNfubg4ADGGP766y+0a9dOYRttbW1oa2vXa3ZCCCGksahcqD09Pev1jbW0tODm5oa4uDh88sknXHtcXBw+/vjjKrfp2bMnjh49iufPn3OTrdy7dw9qampo27ZtveYjhBBChEDlQn3r1i04OTlBTU0Nt27dqrZv586dVdpnSEgIxo4dC3d3d3h4eGD79u3IzMzE1KlTAVSctn706BG3IteoUaOwbNky+Pn5YcmSJcjLy8Ps2bMxceJEpYPJCCGEkKZM5ULdtWtX5OTkwMTEBF27doVIJAJjTKFfTa5Rf/bZZ8jPz8fSpUuRnZ0NJycnxMbGcstjZmdnIzMzk+vfokULxMXFITAwEO7u7mjdujV8fX2xfPlyVT8GIYQQ0qSIWFXVtgoPHz6ElZUVRCIRHj58WG1fIa9DXVhYCH19fRQUFKg0Op2QurCZ958q2zPEo5RvFKa4hCwhpHmpSS1S+Yj61eIr5EJMCCGENCc1WpTjVSkpKdi4cSOSk5MhEonQsWNHBAYGokOHDvWZjxBCCHmr1WpRju+//x5OTk5ISEhAly5d0LlzZyQmJsLJyQlHjx6t74yEEELIW6tWR9Rz5sxBaGgoli5dKte+ePFizJ07F8OHD6+XcIQQQsjbrlZH1Dk5ORg3bpxC+5gxY5RO/0kIIYSQmqtVofby8qpy3eiLFy+iV69edQ5FCCGEkAoqn/r+6aefuD8PGTIEc+fORUJCArp37w4AuHLlCo4ePUoLYBBCCCH1SOX7qNXUVDv4rumiHI2N7qMmjYnuoyaEVKVB7qOWyWR1DkYIIYSQmqnVNWpCCCGENI5aT3jy4sULnD9/HpmZmSgrK5N7LSgoqM7BCCGEEFLLQn3jxg189NFHKC4uxosXL2BoaIi8vDxIJBKYmJhQoSaEEELqSa1Ofc+YMQODBw/GkydPoKOjgytXruDhw4dwc3PDmjVr6jsjIYQQ8taqVaFOSkrCzJkzoa6uDnV1dZSWlsLS0hJff/015s+fX98ZCSGEkLdWrQq1pqYmRCIRAMDU1JRbM1pfX19u/WhCCCGE1E2trlG7uLjg+vXraN++Pfr06YNFixYhLy8P+/fvh7Ozc31nJIQQQt5atTqiXrFiBczNzQEAy5YtQ+vWrfH5558jNzcX27dvr9eAhBBCyNusVkfU7u7u3J+NjY0RGxtbb4EIIYQQ8j+1vo8aAHJzc5GSkgKRSIQOHTrA2Ni4vnIRQgghBLU89V1YWIixY8eiTZs28PT0RO/evWFhYYExY8agoIDmKSaEEELqS60KdUBAAK5evYqTJ0/i2bNnKCgowMmTJ3H9+nVMmjSpvjMSQgghb61anfr+z3/+g9OnT+P999/n2nx8fLBjxw58+OGH9RaOEEIIedvV6oi6devW0NfXV2jX19dHq1at6hyKEEIIIRVqVai/+uorhISEIDs7m2vLycnB7NmzsXDhwnoLRwghhLztVD717eLiws1GBgD379+HtbU1rKysAACZmZnQ1tbGP//8gylTptR/UkIIIeQtpHKhHjp0aAPGIIQQQkhVVC7UixcvbsgchBBCCKlCnSY8SUhIQHJyMkQiERwdHeHi4lJfuQghhBCCWhbq3NxcjBgxAufOnYOBgQEYYygoKECfPn3w3Xff0QxlhBBCSD2p1ajvwMBAFBYW4o8//sCTJ0/w9OlT3LlzB4WFhQgKCqrRvrZs2QJbW1uIxWK4ubkhPj5epe1+++03aGhooGvXrrX4BIQQQkjTUKtCferUKWzduhUODg5cm6OjIzZv3oz//ve/Ku/n8OHDCA4OxoIFC3Djxg306tULAwYMeOOa1gUFBRg3bhz69etXm/iEEEJIk1GrQi2TyaCpqanQrqmpCZlMpvJ+IiIi4O/vj4CAADg4OCAyMhKWlpbYunVrtdtNmTIFo0aNgoeHR42zE0IIIU1JrQp13759MX36dDx+/Jhre/ToEWbMmKHyUW5ZWRkSEhLg7e0t1+7t7Y1Lly4p3W7Pnj148OCByqPQS0tLUVhYKPcghBBCmopaFepNmzahqKgINjY2eOedd2Bvbw9bW1sUFRVh48aNKu0jLy8PUqkUpqamcu2mpqbIycmpcpv79+9j3rx5OHjwIDQ0VBsHt3LlSujr63MPS0tLlbYjhBBChKBWo74tLS2RmJiIuLg4/Pnnn2CMwdHREf3796/xvl6d7QwAGGMKbQAglUoxatQoLFmyBO3bt1d5/6GhoQgJCeGeFxYWUrEmhBDSZNS4UJeXl0MsFiMpKQkffPABPvjgg1q9sZGREdTV1RWOnnNzcxWOsgGgqKgI169fx40bN/Dll18CqLhWzhiDhoYGzpw5g759+ypsp62tDW1t7VplJIQQQvhW41PfGhoasLa2hlQqrdMba2lpwc3NDXFxcXLtcXFx6NGjh0J/PT093L59G0lJSdxj6tSp6NChA5KSktCtW7c65SGEEEKEqFanvr/66iuEhobiwIEDMDQ0rPWbh4SEYOzYsXB3d4eHhwe2b9+OzMxMTJ06FUDFaetHjx5h3759UFNTg5OTk9z2JiYmEIvFCu2EEEJIc1GrQr1hwwakpqbCwsIC1tbW0NXVlXs9MTFRpf189tlnyM/Px9KlS5GdnQ0nJyfExsbC2toaAJCdnf3Ge6oJIYSQ5kzEGGM13WjJkiUQiURQtqmQF/AoLCyEvr4+CgoKoKenx3cc0szZzPtPle0Z4lHKNworaKA0hBChqEktqtERdXFxMWbPno2YmBi8fPkS/fr1w8aNG2FkZFSnwIQQQgipWo0Gky1evBjR0dEYOHAgRo4cibNnz+Lzzz9vqGyEEELIW69GR9THjx/Hrl27MGLECADA6NGj0bNnT0ilUqirqzdIQEIIIcKg9FLOqoGNnOTtUqMj6qysLPTq1Yt7/t5770FDQ0NuKlFCCCGE1J8aFWqpVAotLS25Ng0NDZSXl9drKEIIIYRUqNGpb8YYJkyYIDfTV0lJCaZOnSp3i9bx48frLyEhhBDyFqtRoR4/frxC25gxY+otDCGEEELk1ahQ79mzp6FyEEIIIaQKtVrmkhBCCCGNgwo1IYQQImBUqAkhhBABo0JNCCGECBgVakIIIUTAqFATQgghAkaFmhBCCBEwKtSEEEKIgFGhJoQQQgSMCjUhhBAiYFSoCSGEEAGjQk0IIYQIGBVqQgghRMCoUBNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBEyD7wCEEHnOe52VvnZ7/O1GTEIIEQI6oiaEEEIEjAo1IYQQImC8F+otW7bA1tYWYrEYbm5uiI+PV9r3+PHj+OCDD2BsbAw9PT14eHjg9OnTjZiWEEIIaVy8XqM+fPgwgoODsWXLFvTs2RNRUVEYMGAA7t69CysrK4X+Fy5cwAcffIAVK1bAwMAAe/bsweDBg3H16lW4uLjw8AkIIYRUh8Zc1B2vR9QRERHw9/dHQEAAHBwcEBkZCUtLS2zdurXK/pGRkZgzZw7effddtGvXDitWrEC7du1w4sSJRk5OCCGENA7eCnVZWRkSEhLg7e0t1+7t7Y1Lly6ptA+ZTIaioiIYGho2RERCCCGEd7yd+s7Ly4NUKoWpqalcu6mpKXJyclTax9q1a/HixQv4+voq7VNaWorS0lLueWFhYe0CE0IIITzgfTCZSCSSe84YU2iryqFDhxAWFobDhw/DxMREab+VK1dCX1+fe1haWtY5MyGEENJYeCvURkZGUFdXVzh6zs3NVTjKft3hw4fh7++PI0eOoH///tX2DQ0NRUFBAffIysqqc3ZCCCGksfBWqLW0tODm5oa4uDi59ri4OPTo0UPpdocOHcKECRPw7bffYuDAgW98H21tbejp6ck9CCGEkKaC19uzQkJCMHbsWLi7u8PDwwPbt29HZmYmpk6dCqDiaPjRo0fYt28fgIoiPW7cOKxfvx7du3fnjsZ1dHSgr6/P2+cghBBCGgqvhfqzzz5Dfn4+li5diuzsbDg5OSE2NhbW1tYAgOzsbGRmZnL9o6KiUF5eji+++AJffPEF1z5+/HhER0c3dnxCCCGkwfG+KMe0adMwbdq0Kl97vfieO3eu4QMRQgghAsL7qG9CCCGEKEeFmhBCCBEwKtSEEEKIgPF+jfptRRPVE0IIUQUdURNCCCECRoWaEEIIETAq1IQQQoiAUaEmhBBCBIwKNSGEECJgVKgJIYQQAaNCTQghhAgYFWpCCCFEwKhQE0IIIQJGhZoQQggRMCrUhBBCiIBRoSaEEEIEjBblIITUGS0yQ5oToX2f6YiaEEIIETAq1IQQQoiA0alvojKhnQ4ihJC3AR1RE0IIIQJGhZoQQggRMDr1XUc28/6j9LWMVQMbMQkhhJDmiI6oCSGEEAGjQk0IIYQIGJ36Js0ajVQnyjTF70ZTzEzqjo6oCSGEEAGjQk0IIYQIGBVqQgghRMB4L9RbtmyBra0txGIx3NzcEB8fX23/8+fPw83NDWKxGHZ2dti2bVsjJSWEEEIaH6+F+vDhwwgODsaCBQtw48YN9OrVCwMGDEBmZmaV/dPT0/HRRx+hV69euHHjBubPn4+goCAcO3askZMTQgghjYPXQh0REQF/f38EBATAwcEBkZGRsLS0xNatW6vsv23bNlhZWSEyMhIODg4ICAjAxIkTsWbNmkZOTgghhDQO3m7PKisrQ0JCAubNmyfX7u3tjUuXLlW5zeXLl+Ht7S3X5uPjg127duHly5fQ1NRssLyEEEKUCNNX/pqtVePlaKZ4K9R5eXmQSqUwNTWVazc1NUVOTk6V2+Tk5FTZv7y8HHl5eTA3N1fYprS0FKWlpdzzgoICAEBhYWFdPwIAQFZarPS16t5D+q+0VtvVB6fFp5W+dmeJj9LX+MxcW3xnVvb9KBQxpdvwnVnZ94O+G/zjOzN9n+svc+V+GFP+s+Mwnjx69IgBYJcuXZJrX758OevQoUOV27Rr146tWLFCru3ixYsMAMvOzq5ym8WLFzMA9KAHPehBD3oI7pGVlfXGesnbEbWRkRHU1dUVjp5zc3MVjpormZmZVdlfQ0MDrVu3rnKb0NBQhISEcM9lMhmePHmC1q1bQyQS1fFTyCssLISlpSWysrKgp6dXr/tuKJS5cVDmxkGZGwdlrjvGGIqKimBhYfHGvrwVai0tLbi5uSEuLg6ffPIJ1x4XF4ePP/64ym08PDxw4sQJubYzZ87A3d1d6fVpbW1taGtry7UZGBjULfwb6OnpCeKLUBOUuXFQ5sZBmRsHZa4bfX19lfrxOuo7JCQEO3fuxO7du5GcnIwZM2YgMzMTU6dOBVBxNDxu3Diu/9SpU/Hw4UOEhIQgOTkZu3fvxq5duzBr1iy+PgIhhBDSoHhdlOOzzz5Dfn4+li5diuzsbDg5OSE2NhbW1tYAgOzsbLl7qm1tbREbG4sZM2Zg8+bNsLCwwIYNG/Dpp5/y9REIIYSQBsX76lnTpk3DtGnTqnwtOjpaoc3T0xOJiYkNnKp2tLW1sXjxYoVT7UJGmRsHZW4clLlxUObGJWJMlbHhhBBCCOED73N9E0IIIUQ5KtSEEEKIgFGhJoQQQgSMCjUhhBAiYFSo66C8vBx79+5VOjc5IYQQUlc06ruOJBIJkpOTuXu/m4IJEyZg4sSJ6N27N99RVGZnZ4dr164pTBX77NkzuLq6Ii0tjadk//PTTz+p3HfIkCENmOTtJpVKcfv2bVhbW6NVq1Z8x2myarL4hFBm+nrdhQsXqn29qfwbyPt91E1dt27dkJSU1KQKdVFREby9vWFpaQk/Pz+MHz8ebdq04TtWtTIyMiCVKq5oU1paikePHvGQSNHQoUPlnotEIrmVcV6dW76qzyIEe/fuhZGREQYOHAgAmDNnDrZv3w5HR0ccOnRIkN/z4OBgODs7w9/fH1KpFJ6enrh06RIkEglOnjwJLy8vviM2SQYGBiqvhyDU73NVf/dN4f/D11GhrqNp06YhJCQEWVlZcHNzg66urtzrnTt35imZcseOHUN+fj4OHDiA6OhoLF68GP3794e/vz8+/vhjQa3r/epR6unTp+XmxpVKpfj5559hY2PDQzJFMpmM+/PZs2cxd+5crFixAh4eHhCJRLh06RK++uorrFixgseU1VuxYgW2bt0KoGL9902bNiEyMhInT57EjBkzcPz4cZ4TKvr+++8xZswYAMCJEyeQnp6OP//8E/v27cOCBQvw22+/8Zywat9//z2OHDmCzMxMlJWVyb0mhEmdfv31V+7PGRkZmDdvHiZMmAAPDw8AFd+PvXv3YuXKlXxFfKOnT5/KPX/58iVu3LiBhQsXIjw8nKdUtfDG9bVItUQikcJDTU2N+29TkJiYyL788ksmFouZkZERCw4OZvfu3eM7FmOs6p9v5UNLS4u1b9+enThxgu+YCjp16sTi4+MV2i9cuMA6duzIQyLV6OjosIcPHzLGGJszZw4bO3YsY4yxO3fuMCMjIz6jKaWtrc0tFThp0iQ2ffp0xhhjaWlprGXLljwmU279+vWsRYsW7IsvvmBaWlpsypQprH///kxfX5/Nnz+f73gK+vbty7799luF9oMHDzJPT8/GD1RH58+fZ66urnzHUBkNJquj9PR0hUdaWhr3X6HLzs7GmTNncObMGairq+Ojjz7CH3/8AUdHR6xbt47veJDJZJDJZLC2tsY///zDPZfJZCgtLUVKSgoGDRrEd0wFDx48qHJlHH19fWRkZDR+IBW1aNEC+fn5ACpWpuvfvz8AQCwW499//+UzmlKmpqa4e/cupFIpTp06xWUuLi6Guro6z+mqtmXLFmzfvh2bNm2ClpYW5syZg7i4OAQFBaGgoIDveAouX74Md3d3hXZ3d3f8/vvvPCSqG2NjY6SkpPAdQ3V8/6ZAGl9ZWRn7/vvv2cCBA5mmpiZzc3NjW7duZYWFhVyfQ4cOMQMDAx5T/k9ZWRnz8vJiKSkpfEdRWa9evVjfvn3Z48ePubbs7GzWv39/1rt3bx6TVW/UqFHM1dWV+fv7M4lEwvLy8hhjjP3444+sU6dOPKer2uLFi5m+vj7r2LEjs7KyYiUlJYwxxnbt2sW6d+/Oc7qq6ejosIyMDMYYY8bGxiwpKYkxxti9e/eYoaEhn9Gq1L59exYSEqLQHhISwtq3b89DItXcvHlT7pGUlMT++9//Mk9PT9ajRw++46mMrlHXg/3792Pbtm1IT0/H5cuXYW1tjcjISNja2ipdW5tP5ubmkMlkGDlyJH7//Xd07dpVoY+Pj0+Dr9utKk1NTdy5c0flgS1CsGvXLgwbNgzW1tawsrICAGRmZqJ9+/aIiYnhN1w1Nm/ejK+++gpZWVk4duwYN8o+ISEBI0eO5Dld1cLCwuDk5ISsrCwMHz6cW3RBXV0d8+bN4zld1czMzJCfnw9ra2tYW1vjypUr6NKlC9LT0+UGIArFunXr8Omnn+L06dPo3r07AODKlSt48OABjh07xnM65bp27aowqBMAunfvjt27d/OUqubo9qw62rp1KxYtWoTg4GCEh4fjzp07sLOzQ3R0NPbu3Ss3IEMo9u3bB19fX4jFYr6jqGzmzJnQ1NTEqlWr+I6iMplMhrNnz+LPP/8EYwyOjo7o379/k/qFo6kpKSlpEt/rgIAAWFpaYvHixdi2bRtCQkLQs2dPXL9+HcOGDcOuXbv4jqjgr7/+wtatW5GcnMx9n6dOnQpLS0u+oyn18OFDuedqamowNjZuEt+RV1GhriNHR0esWLECQ4cORcuWLXHz5k3Y2dnhzp078PLyQl5eHt8R5ZSXl0MsFiMpKQlOTk58x1FZYGAg9u3bB3t7e7i7uyuMro+IiOApmaKm+jOuFB8fj6ioKKSlpeHo0aNo06YN9u/fD1tbW7z//vt8x1MglUqxYsUKbNu2DX///Tfu3bsHOzs7LFy4EDY2NvD39+c7ooLKcRYaGhUnNY8cOYKLFy/C3t4eU6dOhZaWFs8J/+fly5fw9vZGVFQU2rdvz3ectxINJquj9PR0uLi4KLRra2vjxYsXPCSqnoaGBqytrZvM/YOV7ty5A1dXV+jp6eHevXu4ceMG90hKSuI7npym+jMGKm7d8/HxgY6ODhITE1FaWgqg4t57od5WFh4ejujoaHz99ddyBc7Z2Rk7d+7kMZlyampqXJEGAF9fX2zYsAFBQUGCKtJA07z09Krz589j8ODBsLe3R7t27TBkyBDEx8fzHatm+Ls83jw4ODiwmJgYxhhjLVq0YA8ePGCMVdx+IdTh/7t372YDBgxg+fn5fEdptprqz7hr165s7969jDH57/ONGzeYqakpn9GUeuedd9jZs2cZY/KZk5OTBTMg8nW2trZswoQJ3MC3Sv/88w+ztbXlKZVyISEhbO7cuXzHqLH9+/czDQ0N5uvry9avX88iIyOZr68v09TUZAcPHuQ7nspoMFkdzZ49G1988QVKSkrAGMPvv/+OQ4cOYeXKlYL9bX7Dhg1ITU2FhYUFrK2tFU4jC2Gyher89ddfEIlEgp5Nran+jFNSUqqcVlFPTw/Pnj1r/EAqePToEezt7RXaZTIZXr58yUOiN8vIyICGhgZ69eqFH3/8Eebm5gAqTuO/fl1VCMrKyrBz507ExcUJ/tLTq8LDw/H1119jxowZXNv06dMRERGBZcuWYdSoUTymUx0V6jry8/NDeXk55syZg+LiYowaNQpt2rTB+vXrMWLECL7jVen1qS6bAplMhuXLl2Pt2rV4/vw5AKBly5aYOXMmFixYADU1YV3FaYo/Y6DijoDU1FSF2d4uXrwIOzs7fkK9QadOnRAfH68wvenRo0ervCwlBCKRCKdOncKsWbPg7u6OmJgYvPvuu3zHUqry0hMA3Lt3T+41IZ8ST0tLw+DBgxXahwwZgvnz5/OQqJb4PqRvTv755x/2999/8x2jWZo3bx4zNjZmW7Zs4e6H3Lx5MzM2NhbkTE5N1erVq5mjoyO7cuUKa9myJYuPj2cHDhxgxsbGbOPGjXzHq9JPP/3E9PX12apVq5hEImHffPMNCwgIYFpaWuzMmTN8x6uSSCTi/q2YN28e09HRYfv372c5OTlNZkbDpuCdd95h27ZtU2jftm0bs7e35yFR7VChrqPi4mL24sUL7nlGRgZbt24dO336NI+p3uzp06dsx44dbN68edx11ISEBPbXX3/xnKxq5ubm7Mcff1Roj4mJYRYWFjwkar7mz5/PdHR0uKlaxWIx++qrr/iOVa1Tp06x3r17M11dXaajo8N69uwp6P8H1dTU5H6p379/PxOLxczPz48KdT3asmUL09LSYlOnTmX79u1j+/fvZ1OmTGHa2tpVFnChotuz6sjb2xvDhg3D1KlT8ezZM3To0AFaWlrIy8tDREQEPv/8c74jKrh16xb69+/PTWeZkpLC3c7y8OFD7Nu3j++ICsRiMW7duqVwe0hKSgq6du0quOktpVIp1q1bp3TRhSdPnvCUTDXFxcW4e/cuZDIZHB0d0aJFC74jNStqamrIycmBiYkJ13b58mV88skn+OeffwR5x8C1a9dw9OjRKr/PQlyspdIPP/yAtWvXIjk5GQDg4OCA2bNnC3IyKqX4/k2hqWvdujW7c+cOY4yxHTt2sM6dOzOpVMqOHDki2MUX+vXrx2bPns0Ykx8l+9tvvzFra2sekyn33nvvscDAQIX2L7/8knXr1o2HRNVbuHAhMzc3Z9988w0Ti8Vs2bJlzN/fn7Vu3ZqtX7+e73jNyoQJE9jZs2eZTCbjO0qd5eTksHPnzvEdQ8GhQ4eYpqYmGzhwINPS0mKDBg1iHTp0YPr6+mzChAl8x1Nq/Pjx7Pz583zHqDMq1HX06mpDw4cPZ2FhYYwxxjIzM5mOjg6f0ZTS09NjqampjDH5Qp2RkcG0tbX5jKbUuXPnmK6uLnNwcGATJ05k/v7+zMHBgbVo0YJduHCB73gK7Ozs2MmTJxljFT/jyp/3+vXr2ciRI/mMVq3nz5+zr776inl4eLB33nmH2drayj2EaPDgwUxbW5tZWFiwkJAQlpiYyHekN1qyZAn7+eefFdqfP3/OlixZwkOi6jk7O7NNmzYxxv73b4ZMJmOTJk1iixYt4jmdcsOGDWPa2trM3t6ehYeHs0ePHvEdqVaoUNeRs7MzW79+PcvMzGR6enrs0qVLjDHGrl+/Ltj7Tk1MTLh/zF4t1KdPn2Zt27blM1q1Hj16xObPn8+GDRvGPvnkE7ZgwQLB/o8nkUi4X+DMzMxYQkICY4yxBw8eMD09PT6jVWvEiBHM3NyczZkzh61bt45FRkbKPYTq6dOnLCoqinl6ejI1NTXm4ODAwsPDWXp6Ot/RqlS5TOvatWvl2oU6mEwikXA/y9atW7Nbt24xxhi7e/cuMzMz4zHZm+Xl5bHIyEjWtWtXpqGhwT788EN25MgRVlZWxnc0lVGhrqOjR48yTU1Npqamxvr378+1r1ixgn344Yc8JlNu0qRJbOjQoaysrIy1aNGCpaWlsYcPHzIXFxduLV8h+OSTT1hBQQFjjLG9e/cqTA4hZO3bt2dXrlxhjDH2/vvvs5UrVzLGGPvuu++YsbExn9Gqpa+vzy5evMh3jDrJyspiX3/9NevYsSNTV1fnO06VRCIR++6775iRkREbP348Ky0tZYwJt1C3bduWK86dO3fm1qa+dOmSoH/xfF1iYiL78ssvmVgsZkZGRiw4OJjdu3eP71hvRIW6HmRnZ7PExEQmlUq5tqtXr7Lk5GQeUylXUFDAevbsyQwMDJi6ujqztLRkmpqarHfv3uz58+d8x+Noampyy0S+PkpW6ObOncvCw8MZYxW/zGloaDB7e3umpaUl6BmebGxs2N27d/mOUWtlZWXshx9+YJ9++ikTi8WCvSOg8vas1NRU5uDgwDw8PFhOTo5gC/XIkSO5o//ly5czY2NjFhAQwKytrdknn3zCczrVPH78mK1atYq1b9+e6erqsnHjxrEPPviAaWhosIiICL7jVYtGfdejpjBj1qt++eUXJCYmQiaTwdXVFf379+c7kpzOnTvD1dUVffr0gZ+fHzZs2AA9Pb0q+44bN66R09XM1atX8dtvv8He3h5DhgzhO45SBw4cwI8//oi9e/dCIpHwHUdlv/76K7799lscO3YMUqkUw4YNw+jRo9G3b1/BTYYDVCzBmZ2dDRMTExQWFsLX1xd//PEHtm3bhiFDhghu1PeTJ09QUlICCwsLyGQyrFmzhltEZOHChWjVqhXfEav08uVL/PTTT9izZw/OnDmDzp07IyAgAKNHj0bLli0BAN999x0+//xzPH36lOe0ylGhrqOmNmMWUDF94eszTwnRb7/9hpkzZ+LBgwd48uQJWrZsWeUsSCKRSPC3OwmZi4uL3M81NTUVjDHY2NhAU1NTrq8Qpz5t27Yt8vPz4ePjg9GjR2Pw4MGCX8bw9duzZDIZgoODsXXrVshkMsEV6qbKyMgIMpkMI0eOxKRJk9C1a1eFPk+fPoWrqyvS09MbP6CKaArROlqwYAF27dqFVatWoWfPnmCM4bfffkNYWBhKSkoQHh7Od0QFdnZ26NGjB8aOHYvhw4fD0NCQ70hV6tmzJ65cuQKg4h+2e/fuyd13KmQWFhbw8vKCl5cXPD090aFDB74jKdVUpzuttGjRIgwfPlywR3VV2bNnD/T19bnnampq2LBhA1xcXHDhwgUek1Vt9OjR3He5KS11uW7dOgwfPrzaX9xatWol6CIN0BF1nVlYWHCnq171448/Ytq0aXj06BFPyZRLTEzEoUOH8N133+Gff/6Bj48PxowZgyFDhkBbW5vveJxhw4YhOjoaenp62Lt3L3x9faGjo8N3LJUcOnQI58+fx7lz53Dv3j2YmprC09OT+8fOwcGB74jNUlO7/NRUTJkyBefPn8e9e/dgZmYGT09P7vvcsWNHvuM1e1So66ipzZj1KsYYzp07J3dt79NPP8Xu3bv5jgYA0NLSwsOHD2Fubi53Ta+p+fvvv/Hrr7/i5MmTOHz4sKBPbV67dg0ymQzdunWTa7969SrU1dXh7u7OUzLlmsrlpw0bNmDy5MkQi8XYsGGD0n4ikQiBgYGNmEx1OTk5OHfuHM6dO8cVbhMTE2RnZ/MdrVmjQl1H3bp1Q7du3RT+xwsMDMS1a9e4U7dCl5iYCH9/f9y6dUswRaSpDyZ7/vw5Ll68yB1Z37hxA46OjvD09MS6dev4jlel9957D3PmzMH//d//ybUfP34cq1evxtWrV3lKplxoaCh27dqFJUuWKFx+mjRpkmAuP9na2uL69eto3bo1bG1tlfYTiURIS0trxGSqe/HiBS5evMgV68TERDg6OuLGjRt8R2vWqFDX0fnz5zFw4EBYWVnBw8MDIpEIly5dQlZWFmJjY9GrVy++IyqVlZWFQ4cO4dtvv8Xt27fh4eGB0aNHC2Z+8kuXLiEkJKRJDibr1q0bbt26BScnJ3h5eaF3797o1asXDAwM+I5WrRYtWuDWrVsKS1qmp6ejc+fOKCoq4imZck3x8tOrKv8JFvJykXPnzsX58+dx8+ZNODk5oXfv3vD09ETv3r0F/51uDmgwWR15enri3r172Lx5M/78808wxjBs2DBMmzYNFhYWfMer0vbt23Hw4EFcvHgRHTt2xOjRoxETEyO4keA9evRosoPJ7t+/D4lEAjs7O9jZ2cHe3r5J/IOmra2Nv//+W6FQZ2dnQ0NDmP9cPHnypMrrpB07dhTcL3Cv2rVrF9atW4f79+8DANq1a4fg4GAEBATwnEzRN998A2NjYyxevBgff/wxjbFoZHRE/RaytLTEiBEjMHr06CpvVxCihw8fIjMzE1FRUUhLS8PRo0fRpk0b7N+/H7a2tnj//ff5jqjg1q1b3LW8+Ph4qKmpwdPTE3369MHUqVP5jlelESNGICcnBz/++CM3KvnZs2cYOnQoTExMcOTIEZ4TKmqKl58WLlyIdevWITAwEB4eHgAqVs/atGkTpk+fjuXLl/OcUN7Nmze5Szjx8fFQV1fnBpN5eXlR4W5gVKhr4datWyr37dy5cwMmqR3GGC5evNikit6xY8cwduxYjB49Gvv378fdu3dhZ2eHLVu24OTJk4iNjeU7YrUSEhKwadMmHDhwQNCDyR49eoTevXsjPz8fLi4uAICkpCSYmpoiLi4OlpaWPCdUpOzyU2ZmJv773/8K8vKTkZERNm7ciJEjR8q1Hzp0CIGBgcjLy+MpmWpu3ryJyMhIwX+fmwthnssSuK5du0IkEuFNv+OIRCJBfoGPHz/OFb3ExESUlpYCAIqKirBixQpBFr3ly5dj27ZtGDduHL777juuvUePHli6dCmPyap248YNbsBNfHw8ioqK0KVLF0yfPh19+vThO55Sbdq0wa1bt3Dw4EHcvHkTOjo68PPzw8iRIxUmPxEKT09PpKSkYOvWrUhOTm4Sl5+kUmmVI+jd3NxQXl7OQ6I3e/07XVhYiK5duwr6+9xc0BF1LTx8+FDlvtbW1g2YpHZcXFwwY8YMjBs3Di1btsTNmzdhZ2eHpKQkfPjhh8jJyeE7ogKJRIK7d+/CxsZGLnNaWhocHR1RUlLCd0Q5GhoacHFx4U4P9u7dW+mIdVJ3JSUluHXrFnJzcyGTyeReE+KUrYGBgdDU1ERERIRc+6xZs/Dvv/9i8+bNPCWrWqtWrfD8+XN06dKFO91N3+nGQ0fUtfBq8V25ciVMTU0xceJEuT67d+/GP//8g7lz5zZ2vDdKSUlB7969Fdr19PTw7Nmzxg+kAnNzc6SmpioMeLt48aLCwCe+SaVSHD9+HO+//75gZ32rzr1793Du3Lkqi96iRYt4SqXcqVOnMG7cOOTn5yuc5RLqWS2gYjDZmTNn0L17dwDAlStXkJWVhXHjxiEkJITr93ox58P+/fupMPOICnUdRUVF4dtvv1Vo79SpE0aMGCHIQt2Uil6lKVOmYPr06di9ezdEIhEeP36My5cvY9asWYIrHurq6vD19UVycnKTK9Q7duzA559/DiMjI5iZmcndMiQSiQT3swaAL7/8EsOHD8eiRYtgamrKdxyV3LlzB66urgCABw8eAACMjY1hbGyMO3fucP2EcsvWoEGDuD/T7G88aJxFupovbW1tlpaWptD+4MEDpq2tzUOiN1u9ejVzdHRkV65cYS1btmTx8fHswIEDzNjYmG3cuJHveErNnz+f6ejoMJFIxEQiEROLxeyrr77iO1aV3N3d2dmzZ/mOUWNWVlZs1apVfMeokZYtW7LU1FS+YzRrUqmULVmyhOnp6TE1NTWmpqbG9PX12dKlS+WW9yUNgwp1Hdnb27P9+/crtO/bt4/Z2trykEg1TanoverFixfs2rVr7OrVq6yoqIjvOEqdPn2ade3alZ04cYI9fvyYFRQUyD2EqmXLluzBgwd8x6gRPz8/tnPnTr5jNGvz5s1jxsbGbMuWLezmzZssKSmJbd68mRkbG7P58+fzHa/Zo8FkdbR69Wp88803+Oabb9C3b18AwM8//4w5c+Zg5syZCA0N5TmhcsXFxbh79y5kMhkcHR3RokULviM1G6/OL/3q6UvGmKCvm/r7++Pdd98V7H3eVSkuLsbw4cNhbGwMZ2dnhdHpQUFBPCVrPpr67G9NHV2jrqM5c+bgyZMnmDZtGsrKygBULNQxd+5cQRdpoGIktRAXWWgOfv31V74j1Iq9vT0WLlyIK1euNJmi9+233+L06dPQ0dHBuXPnFK6rCzFzU9NUZ39rLuiIup48f/4cycnJ0NHRQbt27QS1XCQhqmqKi0WYmZkhKCgI8+bNE8xKWc1NU5z9rTmhQk1IA3n27Bl27dqF5ORkiEQiODo6YuLEidzUnKR+GBoa4tq1a3jnnXf4jtJsNeXFh5oDKtSENIDr16/Dx8cHOjo6eO+998AYw/Xr1/Hvv//izJkz3K05QhASEoJly5ZBV1dX7v7d14lEIqxdu7YRk6lmxowZMDY2xvz58/mO0mxlZmZCQ0NDbvEhR0dHTJs2DeXl5bCysuI7YrNGhZqQBtCrVy/Y29tjx44d3KpT5eXlCAgIQFpaGi5cuMBzwv/p06cPfvjhBxgYGFQ7HaRIJMIvv/zSiMlUExQUhH379qFLly7o3LmzwnV1IUwY0tSpq6sjOztbYfW6/Px8mJiYCHZwZHNBhZqQBqCjo4MbN24oDMC5e/cu3N3dUVxczFOy5qcp/nLR1KipqSEnJ0ehUD98+BCOjo548eIFT8neDjTqm5AGoKenh8zMTIVCnZWVhZYtW/KUqnlqqiPsm4LKSyGVs9JJJBLuNalUiqtXrzaZpXKbMirUhDSAzz77DP7+/lizZg169OgBkUiEixcvYvbs2QpLGxIiVDdu3ABQcf//7du3oaWlxb2mpaWFLl26YNasWXzFe2vQqW9C6smtW7fg5OQENTU1lJWVYfbs2di2bRu3bKGmpiY+//xzrFq1im7fI02Kn58f1q9fT4ty8IQKNSH15NUBN3Z2drh27Rp0dHSQmpoKoGIykVdPHRJCiCro1Dch9cTAwADp6ekwMTFBRkYGZDIZJBIJOnfuzHc0QkgTRoWakHry6aefwtPTE+bm5hCJRHB3d4e6unqVfYU4wxchRJioUBNST7Zv345hw4YhNTUVQUFBmDRpEo3wJoTUGV2jJqQB+Pn5YcOGDVSoCSF1RoWaEEIIETBaaoYQQggRMCrUhBBCiIBRoSaEEEIEjAo1IYQQImBUqAkhhBABo0JNCCGECBgVakIIIUTAqFATQgghAvb/AICpFbMjZVPRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to my surprise, a little it was the\n",
      "\"Ah enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
